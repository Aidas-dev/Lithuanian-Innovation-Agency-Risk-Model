{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8780e802f938c97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T13:34:45.381084Z",
     "start_time": "2025-11-09T13:34:45.379395Z"
    }
   },
   "source": [
    "## Importing the libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778a637b838440c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:51:25.620827Z",
     "start_time": "2025-11-11T14:51:25.339832Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Union, List, Dict\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import os\n",
    "import re\n",
    "import xlsxwriter\n",
    "\n",
    "from src.python.Functions import save_df_list_to_csv_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b622c9ade013314",
   "metadata": {},
   "source": [
    "## Importing the datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20bf31bc77de70f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:58:22.590341Z",
     "start_time": "2025-11-11T14:51:25.650804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing dataset of balance and P&L function:\n",
    "# Execution might take 10 minutes or more due to the large size of the datasets.\n",
    "# Cannot make pandas read csv without errors so we use openpyxl to read xlsx files.\n",
    "\n",
    "balance2025 = pd.read_excel('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2025.xlsx')\n",
    "balance2024 = pd.read_excel('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2024.xlsx')\n",
    "balance2023 = pd.read_excel('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2023.xlsx')\n",
    "balance2022 = pd.read_excel('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2022.xlsx')\n",
    "balance2021 = pd.read_excel('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2021.xlsx')\n",
    "balance2020 = pd.read_excel('../../data/raw/firm-balance-statements/JAR_FA_RODIKLIAI_BLNS_2020.xlsx')\n",
    "\n",
    "pnl2025 = pd.read_excel('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2025.xlsx')\n",
    "pnl2024 = pd.read_excel('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2024.xlsx')\n",
    "pnl2023 = pd.read_excel('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2023.xlsx')\n",
    "pnl2022 = pd.read_excel('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2022.xlsx')\n",
    "pnl2021 = pd.read_excel('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2021.xlsx')\n",
    "pnl2020 = pd.read_excel('../../data/raw/firm-PnL-statements/JAR_FA_RODIKLIAI_PLNA_2020.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863e5bf1b1e5f22",
   "metadata": {},
   "source": [
    "### Fallbacks for large dfs:"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4f6153048a8682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:46:45.107968Z",
     "start_time": "2025-11-11T15:46:44.908518Z"
    }
   },
   "source": [
    "\n",
    "# Fallbacks for large dfs:\n",
    "B2025 = balance2025.copy()\n",
    "B2024 = balance2024.copy()\n",
    "B2023 = balance2023.copy()\n",
    "B2022 = balance2022.copy()\n",
    "B2021 = balance2021.copy()\n",
    "B2020 = balance2020.copy()\n",
    "\n",
    "P2025 = pnl2025.copy()\n",
    "P2024 = pnl2024.copy()\n",
    "P2023 = pnl2023.copy()\n",
    "P2022 = pnl2022.copy()\n",
    "P2021 = pnl2021.copy()\n",
    "P2020 = pnl2020.copy()\n",
    "\n",
    "# Saving in lists for functions:\n",
    "balance_list = [B2025, B2024, B2023, B2022, B2021, B2020]\n",
    "pnl_list = [P2025, P2024, P2023, P2022, P2021, P2020]"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "f27eec3673db73cd",
   "metadata": {},
   "source": [
    "## Cleaning the data with functions:\n",
    "#### Removing unnecessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f6d2875c38ddb353",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:47:34.427215Z",
     "start_time": "2025-11-11T15:47:34.413442Z"
    }
   },
   "source": [
    "# Unnecessary column removal from list of dataframes:\n",
    "def remove_mutual_unnecessary_columns(df_list):\n",
    "    for df in df_list:\n",
    "        remove_columns = ['ja_pavadinimas', 'obj_pav','form_pav','template_name',  'standard_name', 'form_pavadinimas','line_type_id', 'stat_pavadinimas', 'stat_pav']\n",
    "        for col in remove_columns:\n",
    "            if col in df.columns:\n",
    "                df.drop(columns=col, inplace=True)\n",
    "    return df_list\n",
    "\n",
    "# Removing unnecessary rows from list of dataframes:\n",
    "remove_mutual_unnecessary_columns(balance_list)\n",
    "remove_mutual_unnecessary_columns(pnl_list)\n",
    "\n",
    "# Renaming collumns to be the same across dataframes:\n",
    "def rename_columns(df_list):\n",
    "    for df in df_list:\n",
    "        # Rename 'obj_kodas' to 'ja_kodas' if it exists\n",
    "        if 'obj_kodas' in df.columns:\n",
    "            df.rename(columns={'obj_kodas': 'ja_kodas'}, inplace=True)\n",
    "\n",
    "        # Rename other columns if they exist\n",
    "        column_mapping = {\n",
    "            'nuosavas_kapitalas': 'NUOSAVAS KAPITALAS',\n",
    "            'mok_sumos_ir_isipareigojimai': 'MOKĖTINOS SUMOS IR KITI ĮSIPAREIGOJIMAI',\n",
    "            'trumpalaikis_turtas': 'TRUMPALAIKIS TURTAS',\n",
    "            'ilgalaikis_turtas': 'ILGALAIKIS TURTAS',\n",
    "            'pelnas_pries_apmokestinima': 'PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ',\n",
    "        'grynasis_pelnas': 'GRYNASIS PELNAS (NUOSTOLIAI)',\n",
    "        'pardavimo_pajamos': 'PARDAVIMO PAJAMOS'\n",
    "        }\n",
    "\n",
    "        for old_col, new_col in column_mapping.items():\n",
    "            if old_col in df.columns:\n",
    "                df.rename(columns={old_col: new_col}, inplace=True)\n",
    "\n",
    "    return df_list\n",
    "\n",
    "rename_columns(balance_list)\n",
    "rename_columns(pnl_list)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[         ja_kodas  form_kodas  stat_kodas template_id standard_id  \\\n",
       " 0       110003978         310           0      FS0329      IST024   \n",
       " 1       110003978         310           0      FS0329      IST024   \n",
       " 2       110003978         310           0      FS0329      IST024   \n",
       " 3       110004884         310           0      FS0718      IST209   \n",
       " 4       110004884         310           0      FS0718      IST209   \n",
       " ...           ...         ...         ...         ...         ...   \n",
       " 423454  307123738         310           0      FS0329      IST024   \n",
       " 423455  307193537         960           0      FS0522      IST024   \n",
       " 423456  307193537         960           0      FS0522      IST024   \n",
       " 423457  307438075         960           0      FS0522      IST118   \n",
       " 423458  307438075         960           0      FS0522      IST118   \n",
       " \n",
       "                                       line_name  reiksme beginning_date  \\\n",
       " 0                             PARDAVIMO PAJAMOS    97545     2024-01-01   \n",
       " 1                  GRYNASIS PELNAS (NUOSTOLIAI)  2397510     2024-01-01   \n",
       " 2       PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ  2755467     2024-01-01   \n",
       " 3         ATASKAITINIŲ METŲ PELNAS (NUOSTOLIAI)   511543     2024-01-01   \n",
       " 4       PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ   511543     2024-01-01   \n",
       " ...                                         ...      ...            ...   \n",
       " 423454  PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ        0     2025-03-17   \n",
       " 423455             GRYNASIS PELNAS (NUOSTOLIAI)        0     2025-06-06   \n",
       " 423456  PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ        0     2025-06-06   \n",
       " 423457             GRYNASIS PELNAS (NUOSTOLIAI)        0     2025-09-18   \n",
       " 423458  PELNAS (NUOSTOLIAI) PRIEš APMOKESTINIMą        0     2025-09-18   \n",
       " \n",
       "        turning_date    reg_date formavimo_data  \n",
       " 0        2024-12-31  2025-05-28     2025-11-03  \n",
       " 1        2024-12-31  2025-05-28     2025-11-03  \n",
       " 2        2024-12-31  2025-05-28     2025-11-03  \n",
       " 3        2024-12-31  2025-05-28     2025-11-03  \n",
       " 4        2024-12-31  2025-05-28     2025-11-03  \n",
       " ...             ...         ...            ...  \n",
       " 423454   2025-06-30  2025-10-31     2025-11-03  \n",
       " 423455   2025-07-22  2025-07-22     2025-11-03  \n",
       " 423456   2025-07-22  2025-07-22     2025-11-03  \n",
       " 423457   2025-10-22  2025-10-24     2025-11-03  \n",
       " 423458   2025-10-22  2025-10-24     2025-11-03  \n",
       " \n",
       " [423459 rows x 11 columns],\n",
       "          ja_kodas  form_kodas  stat_kodas template_id standard_id  \\\n",
       " 0       110003978         310           0      FS0229      IST024   \n",
       " 1       110003978         310           0      FS0229      IST024   \n",
       " 2       110003978         310           0      FS0229      IST024   \n",
       " 3       110004884         310           0      FS0618      IST209   \n",
       " 4       110004884         310           0      FS0618      IST209   \n",
       " ...           ...         ...         ...         ...         ...   \n",
       " 406750  306748060         960           7      FS0522      IST118   \n",
       " 406751  306748060         960           7      FS0522      IST118   \n",
       " 406752  306748060         960           7      FS0522      IST118   \n",
       " 406753  306815114         310           0      FS0229      IST024   \n",
       " 406754  306815114         310           0      FS0229      IST024   \n",
       " \n",
       "                                       line_name  reiksme beginning_date  \\\n",
       " 0       PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ   -17980     2023-01-01   \n",
       " 1                  GRYNASIS PELNAS (NUOSTOLIAI)   -20409     2023-01-01   \n",
       " 2                             PARDAVIMO PAJAMOS    74824     2023-01-01   \n",
       " 3                             PARDAVIMO PAJAMOS  9701655     2023-01-01   \n",
       " 4       PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ  2151794     2023-01-01   \n",
       " ...                                         ...      ...            ...   \n",
       " 406750                        PARDAVIMO PAJAMOS       72     2024-05-08   \n",
       " 406751             GRYNASIS PELNAS (NUOSTOLIAI)     -198     2024-05-08   \n",
       " 406752  PELNAS (NUOSTOLIAI) PRIEš APMOKESTINIMą     -198     2024-05-08   \n",
       " 406753  PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ    -2071     2024-06-03   \n",
       " 406754             GRYNASIS PELNAS (NUOSTOLIAI)    -2071     2024-06-03   \n",
       " \n",
       "        turning_date    reg_date formavimo_data  \n",
       " 0        2023-12-31  2024-05-23     2025-04-21  \n",
       " 1        2023-12-31  2024-05-23     2025-04-21  \n",
       " 2        2023-12-31  2024-05-23     2025-04-21  \n",
       " 3        2023-12-31  2024-06-03     2025-04-21  \n",
       " 4        2023-12-31  2024-06-03     2025-04-21  \n",
       " ...             ...         ...            ...  \n",
       " 406750   2024-12-11  2024-12-19     2025-04-21  \n",
       " 406751   2024-12-11  2024-12-19     2025-04-21  \n",
       " 406752   2024-12-11  2024-12-19     2025-04-21  \n",
       " 406753   2024-06-30  2024-11-29     2025-04-21  \n",
       " 406754   2024-06-30  2024-11-29     2025-04-21  \n",
       " \n",
       " [406755 rows x 11 columns],\n",
       "          ja_kodas  form_kodas  stat_statusas template_id standard_id  \\\n",
       " 0       110003978         310              2      FS0229      IST024   \n",
       " 1       110004884         310              0      FS0618      IST209   \n",
       " 2       110005648         310              0      FS0229      IST024   \n",
       " 3       110006892         310              0      FS0229      IST024   \n",
       " 4       110008377         310              0      FS0229      IST024   \n",
       " ...           ...         ...            ...         ...         ...   \n",
       " 141939  306370963         310              0      FS0422      IST118   \n",
       " 141940  306372583         960             10      FS0422      IST118   \n",
       " 141941  306384240         310              0      FS0229      IST024   \n",
       " 141942  306453633         960              0      FS0422      IST118   \n",
       " 141943  306600492         960              0      FS0422      IST118   \n",
       " \n",
       "        laikotarpis_nuo laikotarpis_iki    reg_date  \\\n",
       " 0           2022-01-01      2022-12-31  2023-05-24   \n",
       " 1           2022-01-01      2022-12-31  2023-06-30   \n",
       " 2           2022-01-01      2022-12-31  2023-05-15   \n",
       " 3           2022-01-01      2022-12-31  2023-06-16   \n",
       " 4           2022-01-01      2022-12-31  2023-05-23   \n",
       " ...                ...             ...         ...   \n",
       " 141939      2023-08-01      2023-11-30  2023-12-20   \n",
       " 141940      2023-08-02      2023-11-06  2023-11-09   \n",
       " 141941      2023-08-21      2023-09-14  2023-09-21   \n",
       " 141942      2023-09-26      2023-12-31  2023-12-31   \n",
       " 141943      2023-10-24      2023-12-28  2023-12-28   \n",
       " \n",
       "         PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ  GRYNASIS PELNAS (NUOSTOLIAI)  \\\n",
       " 0                                       15420.0                       13104.0   \n",
       " 1                                     -824210.0                           NaN   \n",
       " 2                                       14067.0                       11944.0   \n",
       " 3                                       14149.0                       13441.0   \n",
       " 4                                      159064.0                      135027.0   \n",
       " ...                                         ...                           ...   \n",
       " 141939                                  12142.0                       12142.0   \n",
       " 141940                                      0.0                           0.0   \n",
       " 141941                                      0.0                           0.0   \n",
       " 141942                                      0.0                           0.0   \n",
       " 141943                                      0.0                           0.0   \n",
       " \n",
       "         PARDAVIMO PAJAMOS formavimo_data  \n",
       " 0                 65196.0     2024-10-03  \n",
       " 1               8359577.0     2024-10-03  \n",
       " 2               1010501.0     2024-10-03  \n",
       " 3                167873.0     2024-10-03  \n",
       " 4                537185.0     2024-10-03  \n",
       " ...                   ...            ...  \n",
       " 141939            59125.0     2024-10-03  \n",
       " 141940                NaN     2024-10-03  \n",
       " 141941                NaN     2024-10-03  \n",
       " 141942                0.0     2024-10-03  \n",
       " 141943                NaN     2024-10-03  \n",
       " \n",
       " [141944 rows x 12 columns],\n",
       "          ja_kodas  form_kodas  stat_statusas template_id standard_id  \\\n",
       " 0       301011561         310              0      FS0129      IST024   \n",
       " 1       303641352         310              0      FS0129      IST024   \n",
       " 2       301684796         310              0      FS0128      IST023   \n",
       " 3       302502531         310              0      FS0129      IST024   \n",
       " 4       304590697         310              0      FS0129      IST024   \n",
       " ...           ...         ...            ...         ...         ...   \n",
       " 123685  305189141         310              0      FS0129      IST024   \n",
       " 123686  304922605         960              0      FS0322      IST118   \n",
       " 123687  125935881         310              0      FS0129      IST024   \n",
       " 123688  300629576         310              0      FS0129      IST024   \n",
       " 123689  301507365         310              0      FS0129      IST024   \n",
       " \n",
       "        laikotarpis_nuo laikotarpis_iki    reg_date  \\\n",
       " 0           2021-01-01      2021-12-31  2022-01-02   \n",
       " 1           2021-01-01      2021-12-31  2022-05-27   \n",
       " 2           2021-01-01      2021-12-31  2022-06-29   \n",
       " 3           2021-01-01      2021-12-31  2022-05-30   \n",
       " 4           2021-01-01      2021-12-31  2022-04-21   \n",
       " ...                ...             ...         ...   \n",
       " 123685      2019-06-14      2019-12-31  2022-11-17   \n",
       " 123686      2021-01-01      2021-12-31  2022-02-25   \n",
       " 123687      2018-01-01      2018-12-31  2022-12-12   \n",
       " 123688      2021-01-01      2021-12-31  2022-06-23   \n",
       " 123689      2021-01-01      2021-12-31  2022-05-10   \n",
       " \n",
       "         PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ  GRYNASIS PELNAS (NUOSTOLIAI)  \\\n",
       " 0                                         -91.0                         -91.0   \n",
       " 1                                           0.0                           0.0   \n",
       " 2                                           NaN                           0.0   \n",
       " 3                                       20107.0                       18647.0   \n",
       " 4                                       78141.0                       74162.0   \n",
       " ...                                         ...                           ...   \n",
       " 123685                                      0.0                           0.0   \n",
       " 123686                                    461.0                         435.0   \n",
       " 123687                                      0.0                           0.0   \n",
       " 123688                                  79761.0                       67797.0   \n",
       " 123689                                  75692.0                       63564.0   \n",
       " \n",
       "         PARDAVIMO PAJAMOS formavimo_data  \n",
       " 0                     NaN     2023-03-01  \n",
       " 1                     NaN     2023-03-01  \n",
       " 2                     0.0     2023-03-01  \n",
       " 3               2163507.0     2023-03-01  \n",
       " 4                242465.0     2023-03-01  \n",
       " ...                   ...            ...  \n",
       " 123685                NaN     2023-03-01  \n",
       " 123686            44632.0     2023-03-01  \n",
       " 123687                NaN     2023-03-01  \n",
       " 123688          1733418.0     2023-03-01  \n",
       " 123689          4485193.0     2023-03-01  \n",
       " \n",
       " [123690 rows x 12 columns],\n",
       "          ja_kodas  form_kodas  stat_statusas template_id standard_id  \\\n",
       " 0       305390322         310              0      FS0128      IST023   \n",
       " 1       305213076         310              0      FS0128      IST023   \n",
       " 2       304088367         960              0      FS0322      IST118   \n",
       " 3       300094102         310              0      FS0129      IST024   \n",
       " 4       303219212         960              0      FS0322      IST118   \n",
       " ...           ...         ...            ...         ...         ...   \n",
       " 110220  302686921         310              1      FS0128      IST023   \n",
       " 110221  303289125         310              7      FS0128      IST023   \n",
       " 110222  305333912         310              0      FS0129      IST024   \n",
       " 110223  304150413         310              0      FS0129      IST024   \n",
       " 110224  303226623         630              0      FS0323      IST117   \n",
       " \n",
       "        laikotarpis_nuo laikotarpis_iki    reg_date  \\\n",
       " 0           2019-12-11      2019-12-31  2021-05-27   \n",
       " 1           2019-07-18      2019-12-31  2021-09-08   \n",
       " 2           2020-01-01      2020-12-31  2021-03-31   \n",
       " 3           2020-01-01      2020-12-31  2021-06-10   \n",
       " 4           2020-01-01      2020-12-31  2021-07-12   \n",
       " ...                ...             ...         ...   \n",
       " 110220      2020-01-01      2020-12-31  2021-05-26   \n",
       " 110221      2020-01-01      2020-12-31  2021-03-30   \n",
       " 110222      2020-01-01      2020-12-31  2021-07-11   \n",
       " 110223      2020-01-01      2020-12-31  2021-05-31   \n",
       " 110224      2020-01-01      2020-12-31  2021-05-25   \n",
       " \n",
       "         PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ  GRYNASIS PELNAS (NUOSTOLIAI)  \\\n",
       " 0                                           NaN                           0.0   \n",
       " 1                                           NaN                       -8042.0   \n",
       " 2                                       20624.0                       19562.0   \n",
       " 3                                       72425.0                       70062.0   \n",
       " 4                                           0.0                           0.0   \n",
       " ...                                         ...                           ...   \n",
       " 110220                                      NaN                      -61659.0   \n",
       " 110221                                      NaN                           0.0   \n",
       " 110222                                  20903.0                       19881.0   \n",
       " 110223                                 -10215.0                      -10215.0   \n",
       " 110224                                      NaN                           NaN   \n",
       " \n",
       "         PARDAVIMO PAJAMOS formavimo_data  \n",
       " 0                     NaN     2023-03-01  \n",
       " 1                     NaN     2023-03-01  \n",
       " 2                 43780.0     2023-03-01  \n",
       " 3                155482.0     2023-03-01  \n",
       " 4                     NaN     2023-03-01  \n",
       " ...                   ...            ...  \n",
       " 110220           584174.0     2023-03-01  \n",
       " 110221                0.0     2023-03-01  \n",
       " 110222           142964.0     2023-03-01  \n",
       " 110223            97068.0     2023-03-01  \n",
       " 110224              376.0     2023-03-01  \n",
       " \n",
       " [110225 rows x 12 columns],\n",
       "         ja_kodas  form_kodas  stat_statusas template_id standard_id  \\\n",
       " 0      304406916         310              0      FS0129      IST024   \n",
       " 1      304064380         310              0      FS0128      IST023   \n",
       " 2      305186985         960              0      FS0322      IST118   \n",
       " 3      302915341         310              0      FS0129      IST024   \n",
       " 4      300846448         310              0      FS0129      IST024   \n",
       " ...          ...         ...            ...         ...         ...   \n",
       " 96610  302797423         310              0      FS0128      IST023   \n",
       " 96611  133553116         310              0      FS0128      IST023   \n",
       " 96612  304074296         310              0      FS0128      IST023   \n",
       " 96613  303199069         310              0      FS0129      IST024   \n",
       " 96614  302478692         310              0      FS0128      IST023   \n",
       " \n",
       "       laikotarpis_nuo laikotarpis_iki    reg_date  \\\n",
       " 0          2019-01-01      2019-12-31  2020-04-30   \n",
       " 1          2018-01-01      2018-12-31  2020-06-14   \n",
       " 2          2019-06-10      2019-12-31  2020-06-10   \n",
       " 3          2019-01-01      2019-12-31  2020-10-01   \n",
       " 4          2018-01-01      2018-12-31  2020-12-29   \n",
       " ...               ...             ...         ...   \n",
       " 96610      2018-01-01      2018-12-31  2020-05-04   \n",
       " 96611      2017-01-01      2017-12-31  2020-02-20   \n",
       " 96612      2019-01-01      2019-12-31  2020-05-14   \n",
       " 96613      2019-01-01      2019-12-31  2020-05-30   \n",
       " 96614      2019-01-01      2019-12-31  2020-05-19   \n",
       " \n",
       "        PELNAS (NUOSTOLIAI) PRIEŠ APMOKESTINIMĄ  GRYNASIS PELNAS (NUOSTOLIAI)  \\\n",
       " 0                                     119356.0                      113384.0   \n",
       " 1                                          NaN                         479.0   \n",
       " 2                                          0.0                           0.0   \n",
       " 3                                      -2748.0                       -2748.0   \n",
       " 4                                          0.0                           0.0   \n",
       " ...                                        ...                           ...   \n",
       " 96610                                      NaN                        3252.0   \n",
       " 96611                                      NaN                        3621.0   \n",
       " 96612                                      NaN                        -318.0   \n",
       " 96613                                  72867.0                       69224.0   \n",
       " 96614                                      NaN                         218.0   \n",
       " \n",
       "        PARDAVIMO PAJAMOS formavimo_data  \n",
       " 0               182596.0     2023-03-01  \n",
       " 1                17406.0     2023-03-01  \n",
       " 2                    NaN     2023-03-01  \n",
       " 3               170934.0     2023-03-01  \n",
       " 4                    NaN     2023-03-01  \n",
       " ...                  ...            ...  \n",
       " 96610           233772.0     2023-03-01  \n",
       " 96611            10580.0     2023-03-01  \n",
       " 96612                0.0     2023-03-01  \n",
       " 96613            75500.0     2023-03-01  \n",
       " 96614           478292.0     2023-03-01  \n",
       " \n",
       " [96615 rows x 12 columns]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "95312fa08ed7ae2e",
   "metadata": {},
   "source": [
    "#### Extracting columns line_name, reiksme and ja_kodas:\n",
    "This function is uneeded but if you need to view just the extracted columns, you can use it:"
   ]
  },
  {
   "cell_type": "code",
   "id": "1d99a3f07c2c9d54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:47:34.584180Z",
     "start_time": "2025-11-11T15:47:34.548312Z"
    }
   },
   "source": [
    "# Extracting columns line_name, reiksme and ja_kodas from dfs with all of this data:\n",
    "def extract_line_name_reiksme_ja_kodas(df_list):\n",
    "    \"\"\"\n",
    "    Extract columns 'line_name', 'reiksme', and 'ja_kodas' from DataFrames\n",
    "    that contain all three columns.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list : list of pandas.DataFrame\n",
    "        List of DataFrames to process (will be modified in-place)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of pandas.DataFrame\n",
    "        List of DataFrames containing only the three specified columns\n",
    "    \"\"\"\n",
    "    extracted_dfs = []\n",
    "\n",
    "    required_columns = ['line_name', 'reiksme', 'ja_kodas']\n",
    "\n",
    "    for i, df in enumerate(df_list):\n",
    "        # Check if all required columns exist in the current DataFrame\n",
    "        if all(col in df.columns for col in required_columns):\n",
    "            # Extract only the required columns\n",
    "            extracted_df = df[required_columns].copy()\n",
    "            extracted_dfs.append(extracted_df)\n",
    "\n",
    "\n",
    "        else:\n",
    "            missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "            print(f\"DataFrame {i}: Missing columns {missing_cols} - skipped\")\n",
    "\n",
    "    print(f\"\\nExtracted {len(extracted_dfs)} out of {len(df_list)} DataFrames\")\n",
    "    return extracted_dfs\n",
    "\n",
    "B_extracted = extract_line_name_reiksme_ja_kodas(balance_list)\n",
    "P_extracted = extract_line_name_reiksme_ja_kodas(pnl_list)\n",
    "\n",
    "# Renaming the extracted dfs to be more descriptive:\n",
    "B_extracted_2025 = B_extracted[0]\n",
    "B_extracted_2024 = B_extracted[1]\n",
    "\n",
    "P_extracted_2025 = P_extracted[0]\n",
    "P_extracted_2024 = P_extracted[1]\n",
    "\n",
    "# New lists with extracted and renamed dfs:\n",
    "B_extracted_renamed = [B_extracted_2025, B_extracted_2024]\n",
    "P_extracted_renamed = [P_extracted_2025, P_extracted_2024]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 2: Missing columns ['line_name', 'reiksme'] - skipped\n",
      "DataFrame 3: Missing columns ['line_name', 'reiksme'] - skipped\n",
      "DataFrame 4: Missing columns ['line_name', 'reiksme'] - skipped\n",
      "DataFrame 5: Missing columns ['line_name', 'reiksme'] - skipped\n",
      "\n",
      "Extracted 2 out of 6 DataFrames\n",
      "DataFrame 2: Missing columns ['line_name', 'reiksme'] - skipped\n",
      "DataFrame 3: Missing columns ['line_name', 'reiksme'] - skipped\n",
      "DataFrame 4: Missing columns ['line_name', 'reiksme'] - skipped\n",
      "DataFrame 5: Missing columns ['line_name', 'reiksme'] - skipped\n",
      "\n",
      "Extracted 2 out of 6 DataFrames\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "30a3db0ea1422c0a",
   "metadata": {},
   "source": [
    "#### Shifting the column data of extraced dfs so that every row is unique with new columns:"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e29f4ecec006ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:47:36.465330Z",
     "start_time": "2025-11-11T15:47:34.735393Z"
    }
   },
   "source": [
    "# Shifting the column data of extraced dfs so that every row is unique:\n",
    "def pivot_dfs(df_list, aggfunc='first'):\n",
    "    \"\"\"\n",
    "    Apply pivot transformation to multiple DataFrames with many columns.\n",
    "    Only uses 'line_name', 'reiksme', 'ja_kodas' for pivoting, ignores other columns.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list : list of pandas.DataFrame\n",
    "        List of DataFrames to pivot (can have many additional columns)\n",
    "    aggfunc : str or function, default 'first'\n",
    "        Aggregation function for duplicates\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of pandas.DataFrame\n",
    "        List of pivoted DataFrames\n",
    "    \"\"\"\n",
    "\n",
    "    def pivot_line_names_to_columns(df, aggfunc='first'):\n",
    "        \"\"\"\n",
    "        Pivot using only the three required columns, ignoring all others.\n",
    "        \"\"\"\n",
    "        # Select only the columns needed for pivoting\n",
    "        pivot_data = df[['ja_kodas', 'line_name', 'reiksme']].copy()\n",
    "\n",
    "        # Pivot the table\n",
    "        pivoted_df = pivot_data.pivot_table(\n",
    "            index='ja_kodas',\n",
    "            columns='line_name',\n",
    "            values='reiksme',\n",
    "            aggfunc=aggfunc\n",
    "        ).reset_index()\n",
    "\n",
    "        # Reset column names and clean up the DataFrame\n",
    "        pivoted_df.columns.name = None\n",
    "\n",
    "        return pivoted_df\n",
    "\n",
    "    pivoted_dfs = []\n",
    "\n",
    "    for i, df in enumerate(df_list):\n",
    "        try:\n",
    "            # Check if required columns exist\n",
    "            required_columns = ['line_name', 'reiksme', 'ja_kodas']\n",
    "            if not all(col in df.columns for col in required_columns):\n",
    "                missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "                print(f\"DataFrame {i}: Missing columns {missing_cols}. Available: {list(df.columns)}\")\n",
    "                pivoted_dfs.append(df)\n",
    "                continue\n",
    "\n",
    "            # Show info about the DataFrame\n",
    "            print(f\"DataFrame {i}: Original columns: {len(df.columns)}\")\n",
    "            print(f\"DataFrame {i}: Using 3 columns for pivoting, ignoring {len(df.columns) - 3} other columns\")\n",
    "            print(f\"DataFrame {i}: Unique line_name values: {df['line_name'].nunique()}\")\n",
    "            print(f\"DataFrame {i}: Unique ja_kodas values: {df['ja_kodas'].nunique()}\")\n",
    "\n",
    "            # Perform pivot (only uses the 3 required columns)\n",
    "            pivoted_df = pivot_line_names_to_columns(df, aggfunc)\n",
    "            pivoted_dfs.append(pivoted_df)\n",
    "            print(f\"DataFrame {i}: Successfully pivoted. Original shape: {df.shape}, Pivoted shape: {pivoted_df.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"DataFrame {i}: Error during pivoting - {e}\")\n",
    "            pivoted_dfs.append(df)\n",
    "\n",
    "    return pivoted_dfs\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0: Original columns: 3\n",
      "DataFrame 0: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 0: Unique line_name values: 9\n",
      "DataFrame 0: Unique ja_kodas values: 146512\n",
      "DataFrame 0: Successfully pivoted. Original shape: (615188, 3), Pivoted shape: (146512, 10)\n",
      "DataFrame 1: Original columns: 3\n",
      "DataFrame 1: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 1: Unique line_name values: 9\n",
      "DataFrame 1: Unique ja_kodas values: 141315\n",
      "DataFrame 1: Successfully pivoted. Original shape: (604271, 3), Pivoted shape: (141315, 10)\n",
      "DataFrame 0: Original columns: 3\n",
      "DataFrame 0: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 0: Unique line_name values: 8\n",
      "DataFrame 0: Unique ja_kodas values: 146512\n",
      "DataFrame 0: Successfully pivoted. Original shape: (423459, 3), Pivoted shape: (146512, 9)\n",
      "DataFrame 1: Original columns: 3\n",
      "DataFrame 1: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 1: Unique line_name values: 8\n",
      "DataFrame 1: Unique ja_kodas values: 141315\n",
      "DataFrame 1: Successfully pivoted. Original shape: (406755, 3), Pivoted shape: (141315, 9)\n",
      "DataFrame 0: Original columns: 3\n",
      "DataFrame 0: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 0: Unique line_name values: 9\n",
      "DataFrame 0: Unique ja_kodas values: 146512\n",
      "DataFrame 0: Successfully pivoted. Original shape: (615188, 3), Pivoted shape: (146512, 10)\n",
      "DataFrame 1: Original columns: 3\n",
      "DataFrame 1: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 1: Unique line_name values: 9\n",
      "DataFrame 1: Unique ja_kodas values: 141315\n",
      "DataFrame 1: Successfully pivoted. Original shape: (604271, 3), Pivoted shape: (141315, 10)\n",
      "DataFrame 0: Original columns: 3\n",
      "DataFrame 0: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 0: Unique line_name values: 9\n",
      "DataFrame 0: Unique ja_kodas values: 146512\n",
      "DataFrame 0: Successfully pivoted. Original shape: (615188, 3), Pivoted shape: (146512, 10)\n",
      "DataFrame 1: Original columns: 3\n",
      "DataFrame 1: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 1: Unique line_name values: 9\n",
      "DataFrame 1: Unique ja_kodas values: 141315\n",
      "DataFrame 1: Successfully pivoted. Original shape: (604271, 3), Pivoted shape: (141315, 10)\n",
      "DataFrame 0: Original columns: 3\n",
      "DataFrame 0: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 0: Unique line_name values: 8\n",
      "DataFrame 0: Unique ja_kodas values: 146512\n",
      "DataFrame 0: Successfully pivoted. Original shape: (423459, 3), Pivoted shape: (146512, 9)\n",
      "DataFrame 1: Original columns: 3\n",
      "DataFrame 1: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 1: Unique line_name values: 8\n",
      "DataFrame 1: Unique ja_kodas values: 141315\n",
      "DataFrame 1: Successfully pivoted. Original shape: (406755, 3), Pivoted shape: (141315, 9)\n",
      "DataFrame 0: Original columns: 3\n",
      "DataFrame 0: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 0: Unique line_name values: 8\n",
      "DataFrame 0: Unique ja_kodas values: 146512\n",
      "DataFrame 0: Successfully pivoted. Original shape: (423459, 3), Pivoted shape: (146512, 9)\n",
      "DataFrame 1: Original columns: 3\n",
      "DataFrame 1: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 1: Unique line_name values: 8\n",
      "DataFrame 1: Unique ja_kodas values: 141315\n",
      "DataFrame 1: Successfully pivoted. Original shape: (406755, 3), Pivoted shape: (141315, 9)\n",
      "DataFrame 0: Original columns: 3\n",
      "DataFrame 0: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 0: Unique line_name values: 9\n",
      "DataFrame 0: Unique ja_kodas values: 146512\n",
      "DataFrame 0: Successfully pivoted. Original shape: (615188, 3), Pivoted shape: (146512, 10)\n",
      "DataFrame 1: Original columns: 3\n",
      "DataFrame 1: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 1: Unique line_name values: 9\n",
      "DataFrame 1: Unique ja_kodas values: 141315\n",
      "DataFrame 1: Successfully pivoted. Original shape: (604271, 3), Pivoted shape: (141315, 10)\n",
      "DataFrame 0: Original columns: 3\n",
      "DataFrame 0: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 0: Unique line_name values: 8\n",
      "DataFrame 0: Unique ja_kodas values: 146512\n",
      "DataFrame 0: Successfully pivoted. Original shape: (423459, 3), Pivoted shape: (146512, 9)\n",
      "DataFrame 1: Original columns: 3\n",
      "DataFrame 1: Using 3 columns for pivoting, ignoring 0 other columns\n",
      "DataFrame 1: Unique line_name values: 8\n",
      "DataFrame 1: Unique ja_kodas values: 141315\n",
      "DataFrame 1: Successfully pivoted. Original shape: (406755, 3), Pivoted shape: (141315, 9)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "369daaa15c18235c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:47:36.735666Z",
     "start_time": "2025-11-11T15:47:36.731718Z"
    }
   },
   "source": [
    "# Pivoting the big dataframes:\n",
    "def pivot_dfs_smart(df_list):\n",
    "    \"\"\"\n",
    "    Apply pivot transformation with smart aggregation for different column types.\n",
    "    \"\"\"\n",
    "\n",
    "    def pivot_line_names_to_columns_smart(df):\n",
    "        \"\"\"\n",
    "        Pivot with intelligent aggregation based on column data types.\n",
    "        \"\"\"\n",
    "        # Identify column types for smart aggregation\n",
    "        numeric_columns = []\n",
    "        string_columns = []\n",
    "\n",
    "        for col in df.columns:\n",
    "            if col in ['line_name', 'reiksme']:\n",
    "                continue\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                numeric_columns.append(col)\n",
    "            else:\n",
    "                string_columns.append(col)\n",
    "\n",
    "        # Create aggregation dictionary\n",
    "        aggregation_dict = {'reiksme': 'first'}\n",
    "\n",
    "        # For numeric columns, use 'first' or 'mean' depending on context\n",
    "        for col in numeric_columns:\n",
    "            aggregation_dict[col] = 'first'\n",
    "\n",
    "        # For string columns, use 'first' (take the first occurrence)\n",
    "        for col in string_columns:\n",
    "            aggregation_dict[col] = 'first'\n",
    "\n",
    "        # Identify index columns (all columns except line_name and reiksme)\n",
    "        index_columns = [col for col in df.columns if col not in ['line_name', 'reiksme']]\n",
    "\n",
    "        # Perform pivot\n",
    "        pivoted_df = df.pivot_table(\n",
    "            index=index_columns,\n",
    "            columns='line_name',\n",
    "            values='reiksme',\n",
    "            aggfunc='first'\n",
    "        ).reset_index()\n",
    "\n",
    "        # Reset column names\n",
    "        pivoted_df.columns.name = None\n",
    "\n",
    "        return pivoted_df\n",
    "\n",
    "    pivoted_dfs = []\n",
    "\n",
    "    for i, df in enumerate(df_list):\n",
    "        try:\n",
    "            required_columns = ['line_name', 'reiksme', 'ja_kodas']\n",
    "            if not all(col in df.columns for col in required_columns):\n",
    "                missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "                print(f\"DataFrame {i}: Missing columns {missing_cols}\")\n",
    "                pivoted_dfs.append(df)\n",
    "                continue\n",
    "\n",
    "            print(f\"DataFrame {i}: Preserving {len(df.columns) - 2} columns in result\")\n",
    "\n",
    "            pivoted_df = pivot_line_names_to_columns_smart(df)\n",
    "            pivoted_dfs.append(pivoted_df)\n",
    "            print(f\"DataFrame {i}: Successfully pivoted. Original shape: {df.shape}, Pivoted shape: {pivoted_df.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"DataFrame {i}: Error during pivoting - {e}\")\n",
    "            pivoted_dfs.append(df)\n",
    "\n",
    "    return pivoted_dfs\n",
    "\n",
    "Bpivoted\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "6da87921479dbecd",
   "metadata": {},
   "source": [
    "### Checking duplicate rows in all dfs:"
   ]
  },
  {
   "cell_type": "code",
   "id": "4d5a00fcbbfcad10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:48:00.121869Z",
     "start_time": "2025-11-11T15:47:36.789600Z"
    }
   },
   "source": [
    "# Checking duplicate rows in all dfs and removing/extracting them:\n",
    "\n",
    "def extract_and_remove_older_duplicates(df_list, date_column='reg_date', id_column='ja_kodas'):\n",
    "    \"\"\"\n",
    "    Find duplicate ja_kodas rows, keep the most recent reg_date, and extract older duplicates.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list : list of pandas.DataFrame\n",
    "        List of DataFrames to process\n",
    "    date_column : str, default 'reg_date'\n",
    "        Name of the date column to determine which rows to keep\n",
    "    id_column : str, default 'ja_kodas'\n",
    "        Name of the ID column to check for duplicates\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (cleaned_dfs, extracted_dfs, summary)\n",
    "        cleaned_dfs: List of DataFrames with only most recent rows kept\n",
    "        extracted_dfs: List of DataFrames containing the older duplicate rows\n",
    "        summary: Dictionary with statistics for each DataFrame\n",
    "    \"\"\"\n",
    "    cleaned_dfs = []\n",
    "    extracted_dfs = []\n",
    "    summary = {}\n",
    "\n",
    "    for i, df in enumerate(df_list):\n",
    "        try:\n",
    "            # Check if required columns exist\n",
    "            if id_column not in df.columns:\n",
    "                print(f\"DataFrame {i}: '{id_column}' column not found. Available: {list(df.columns)}\")\n",
    "                cleaned_dfs.append(df)\n",
    "                extracted_dfs.append(pd.DataFrame())  # Empty DataFrame for consistency\n",
    "                summary[i] = {'total_rows': len(df), 'duplicates_found': 0, 'kept': len(df), 'extracted': 0}\n",
    "                continue\n",
    "\n",
    "            if date_column not in df.columns:\n",
    "                print(f\"DataFrame {i}: '{date_column}' column not found. Available: {list(df.columns)}\")\n",
    "                cleaned_dfs.append(df)\n",
    "                extracted_dfs.append(pd.DataFrame())\n",
    "                summary[i] = {'total_rows': len(df), 'duplicates_found': 0, 'kept': len(df), 'extracted': 0}\n",
    "                continue\n",
    "\n",
    "            # Make a copy to avoid modifying original\n",
    "            df_working = df.copy()\n",
    "\n",
    "            # Ensure date_column is datetime\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df_working[date_column]):\n",
    "                df_working[date_column] = pd.to_datetime(df_working[date_column], errors='coerce')\n",
    "                # Remove rows where date conversion failed\n",
    "                invalid_dates = df_working[date_column].isna().sum()\n",
    "                if invalid_dates > 0:\n",
    "                    print(f\"DataFrame {i}: {invalid_dates} rows with invalid dates will be treated as oldest\")\n",
    "\n",
    "            # Find duplicate ja_kodas\n",
    "            duplicate_mask = df_working.duplicated(subset=[id_column], keep=False)\n",
    "            duplicate_rows = df_working[duplicate_mask]\n",
    "\n",
    "            if len(duplicate_rows) == 0:\n",
    "                print(f\"DataFrame {i}: No duplicate {id_column} found\")\n",
    "                cleaned_dfs.append(df_working)\n",
    "                extracted_dfs.append(pd.DataFrame())\n",
    "                summary[i] = {'total_rows': len(df), 'duplicates_found': 0, 'kept': len(df), 'extracted': 0}\n",
    "                continue\n",
    "\n",
    "            # Group by ja_kodas and find the most recent date for each\n",
    "            duplicate_groups = duplicate_rows.groupby(id_column)\n",
    "\n",
    "            # Identify rows to keep (most recent date) and extract (older dates)\n",
    "            keep_indices = []\n",
    "            extract_indices = []\n",
    "\n",
    "            for ja_kodas, group in duplicate_groups:\n",
    "                if len(group) > 1:\n",
    "                    # Find the row with the most recent date\n",
    "                    most_recent_idx = group[date_column].idxmax()\n",
    "                    keep_indices.append(most_recent_idx)\n",
    "\n",
    "                    # Extract all other rows (older dates)\n",
    "                    older_indices = group.index[group.index != most_recent_idx].tolist()\n",
    "                    extract_indices.extend(older_indices)\n",
    "\n",
    "            # Create DataFrames\n",
    "            keep_df = df_working[~df_working.index.isin(extract_indices)]  # All non-extracted rows\n",
    "            extract_df = df_working[df_working.index.isin(extract_indices)]  # Only extracted rows\n",
    "\n",
    "            # Also include non-duplicate rows in keep_df\n",
    "            non_duplicate_rows = df_working[~duplicate_mask]\n",
    "            final_keep_df = pd.concat([keep_df, non_duplicate_rows], ignore_index=True)\n",
    "\n",
    "            cleaned_dfs.append(final_keep_df)\n",
    "            extracted_dfs.append(extract_df)\n",
    "\n",
    "            # Summary statistics\n",
    "            summary[i] = {\n",
    "                'total_rows': len(df),\n",
    "                'duplicates_found': len(duplicate_rows),\n",
    "                'unique_duplicate_ids': duplicate_rows[id_column].nunique(),\n",
    "                'kept': len(final_keep_df),\n",
    "                'extracted': len(extract_df),\n",
    "                'example_extracted': extract_df[id_column].value_counts().head(3).to_dict() if len(extract_df) > 0 else {}\n",
    "            }\n",
    "\n",
    "            print(f\"DataFrame {i}: Kept {len(final_keep_df)} rows, extracted {len(extract_df)} older duplicates\")\n",
    "            if len(extract_df) > 0:\n",
    "                print(f\"  Extracted {extract_df[id_column].nunique()} unique {id_column}s with older dates\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"DataFrame {i}: Error processing - {e}\")\n",
    "            cleaned_dfs.append(df)\n",
    "            extracted_dfs.append(pd.DataFrame())\n",
    "            summary[i] = {'total_rows': len(df), 'duplicates_found': 0, 'kept': len(df), 'extracted': 0, 'error': str(e)}\n",
    "\n",
    "    # Final summary\n",
    "    total_extracted = sum(s['extracted'] for s in summary.values())\n",
    "    total_duplicates = sum(s['duplicates_found'] for s in summary.values())\n",
    "\n",
    "    print(f\"\\n=== EXTRACTION SUMMARY ===\")\n",
    "    print(f\"Total DataFrames processed: {len(df_list)}\")\n",
    "    print(f\"Total duplicate rows found: {total_duplicates}\")\n",
    "    print(f\"Total older rows extracted: {total_extracted}\")\n",
    "\n",
    "    return cleaned_dfs, extracted_dfs, summary\n",
    "\n",
    "# Balance data:\n",
    "cleaned_dfs, extracted_dfs, summary = extract_and_remove_older_duplicates(\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0: Kept 147160 rows, extracted 468676 older duplicates\n",
      "  Extracted 145864 unique ja_kodass with older dates\n",
      "DataFrame 1: Kept 142162 rows, extracted 462956 older duplicates\n",
      "  Extracted 140468 unique ja_kodass with older dates\n",
      "DataFrame 2: Kept 175078 rows, extracted 152850 older duplicates\n",
      "  Extracted 128160 unique ja_kodass with older dates\n",
      "DataFrame 3: Kept 227434 rows, extracted 21325 older duplicates\n",
      "  Extracted 10710 unique ja_kodass with older dates\n",
      "DataFrame 4: Kept 204568 rows, extracted 14073 older duplicates\n",
      "  Extracted 8982 unique ja_kodass with older dates\n",
      "DataFrame 5: Kept 187052 rows, extracted 9965 older duplicates\n",
      "  Extracted 5652 unique ja_kodass with older dates\n",
      "\n",
      "=== EXTRACTION SUMMARY ===\n",
      "Total DataFrames processed: 6\n",
      "Total duplicate rows found: 1569681\n",
      "Total older rows extracted: 1129845\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "f80e539a2a1c4eac",
   "metadata": {},
   "source": [
    "## Adding PnL data to balance data:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e5497bebf8372e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:48:00.557388Z",
     "start_time": "2025-11-11T15:48:00.551335Z"
    }
   },
   "source": [
    "def merge_pnl_and_balances(df_list1, df_list2, how='inner'):\n",
    "    \"\"\"\n",
    "    Merge two lists of DataFrames on ja_kodas column with comprehensive diagnostics and validation.\n",
    "    Each DataFrame from list1 is merged with corresponding DataFrame from list2.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_list1, df_list2 : list of pandas.DataFrame\n",
    "        Lists of DataFrames to merge\n",
    "    how : str, default 'inner'\n",
    "        Type of merge: 'inner', 'left', 'right', 'outer'\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of pandas.DataFrame\n",
    "        List of merged DataFrames\n",
    "    \"\"\"\n",
    "    if len(df_list1) != len(df_list2):\n",
    "        print(f\"Warning: List lengths differ - list1: {len(df_list1)}, list2: {len(df_list2)}\")\n",
    "        # Use the minimum length to avoid index errors\n",
    "        min_length = min(len(df_list1), len(df_list2))\n",
    "        df_list1 = df_list1[:min_length]\n",
    "        df_list2 = df_list2[:min_length]\n",
    "\n",
    "    merged_dfs = []\n",
    "\n",
    "    for i, (df1, df2) in enumerate(zip(df_list1, df_list2)):\n",
    "        try:\n",
    "            # Check if ja_kodas exists in both DataFrames\n",
    "            if 'ja_kodas' not in df1.columns:\n",
    "                print(f\"Pair {i}: ja_kodas not found in first DataFrame, skipping\")\n",
    "                continue\n",
    "            if 'ja_kodas' not in df2.columns:\n",
    "                print(f\"Pair {i}: ja_kodas not found in second DataFrame, skipping\")\n",
    "                continue\n",
    "\n",
    "            # Create copies to avoid modifying originals\n",
    "            df1_clean = df1.copy()\n",
    "            df2_clean = df2.copy()\n",
    "\n",
    "            # Validate ja_kodas data types and convert if necessary\n",
    "            if df1_clean['ja_kodas'].dtype != df2_clean['ja_kodas'].dtype:\n",
    "                print(f\"Pair {i}: ja_kodas data types differ - converting both to string\")\n",
    "                df1_clean['ja_kodas'] = df1_clean['ja_kodas'].astype(str)\n",
    "                df2_clean['ja_kodas'] = df2_clean['ja_kodas'].astype(str)\n",
    "\n",
    "            # Check for duplicate ja_kodas within each DataFrame\n",
    "            df1_duplicates = df1_clean.duplicated(subset=['ja_kodas']).sum()\n",
    "            df2_duplicates = df2_clean.duplicated(subset=['ja_kodas']).sum()\n",
    "\n",
    "            if df1_duplicates > 0:\n",
    "                print(f\"Pair {i}: WARNING - {df1_duplicates} duplicate ja_kodas found in first DataFrame\")\n",
    "                # Keep first occurrence of duplicates\n",
    "                df1_clean = df1_clean.drop_duplicates(subset=['ja_kodas'], keep='first')\n",
    "\n",
    "            if df2_duplicates > 0:\n",
    "                print(f\"Pair {i}: WARNING - {df2_duplicates} duplicate ja_kodas found in second DataFrame\")\n",
    "                # Keep first occurrence of duplicates\n",
    "                df2_clean = df2_clean.drop_duplicates(subset=['ja_kodas'], keep='first')\n",
    "\n",
    "            # Check for NaN values in ja_kodas\n",
    "            df1_nan = df1_clean['ja_kodas'].isna().sum()\n",
    "            df2_nan = df2_clean['ja_kodas'].isna().sum()\n",
    "\n",
    "            if df1_nan > 0:\n",
    "                print(f\"Pair {i}: WARNING - {df1_nan} NaN values in ja_kodas (first DataFrame), removing\")\n",
    "                df1_clean = df1_clean.dropna(subset=['ja_kodas'])\n",
    "\n",
    "            if df2_nan > 0:\n",
    "                print(f\"Pair {i}: WARNING - {df2_nan} NaN values in ja_kodas (second DataFrame), removing\")\n",
    "                df2_clean = df2_clean.dropna(subset=['ja_kodas'])\n",
    "\n",
    "            # Pre-merge diagnostics\n",
    "            df1_unique = df1_clean['ja_kodas'].nunique()\n",
    "            df2_unique = df2_clean['ja_kodas'].nunique()\n",
    "            common_ja_kodas = set(df1_clean['ja_kodas']) & set(df2_clean['ja_kodas'])\n",
    "            common_count = len(common_ja_kodas)\n",
    "\n",
    "            print(f\"\\n--- Pair {i} Merge Diagnostics ---\")\n",
    "            print(f\"DF1: {len(df1_clean)} rows, {df1_unique} unique ja_kodas\")\n",
    "            print(f\"DF2: {len(df2_clean)} rows, {df2_unique} unique ja_kodas\")\n",
    "            print(f\"Common ja_kodas: {common_count}\")\n",
    "            print(f\"Merge type: {how}\")\n",
    "\n",
    "            # Calculate expected result sizes\n",
    "            if how == 'inner':\n",
    "                expected_rows = common_count\n",
    "            elif how == 'left':\n",
    "                expected_rows = len(df1_clean)\n",
    "            elif how == 'right':\n",
    "                expected_rows = len(df2_clean)\n",
    "            else:  # outer\n",
    "                expected_rows = len(df1_clean) + len(df2_clean) - common_count\n",
    "\n",
    "            print(f\"Expected result rows: {expected_rows}\")\n",
    "\n",
    "            # Perform the merge with indicator to track sources\n",
    "            merged_df = pd.merge(\n",
    "                df1_clean,\n",
    "                df2_clean,\n",
    "                on='ja_kodas',\n",
    "                how=how,\n",
    "                suffixes=('_pnl', '_balance'),\n",
    "                indicator=True  # Add merge indicator column\n",
    "            )\n",
    "\n",
    "            # Post-merge diagnostics\n",
    "            actual_rows = len(merged_df)\n",
    "            merge_stats = merged_df['_merge'].value_counts()\n",
    "\n",
    "            print(f\"Actual result rows: {actual_rows}\")\n",
    "            print(f\"Merge composition: {merge_stats.to_dict()}\")\n",
    "\n",
    "            if actual_rows != expected_rows:\n",
    "                print(f\"WARNING: Expected {expected_rows} rows but got {actual_rows} rows\")\n",
    "\n",
    "            # Remove the indicator column\n",
    "            merged_df = merged_df.drop('_merge', axis=1)\n",
    "\n",
    "            # Check for overlapping column names (besides ja_kodas)\n",
    "            overlapping_cols = set(df1_clean.columns) & set(df2_clean.columns) - {'ja_kodas'}\n",
    "            if overlapping_cols:\n",
    "                print(f\"Overlapping columns (received suffixes): {list(overlapping_cols)}\")\n",
    "\n",
    "            print(f\"Pair {i}: Successfully merged. Shapes: {df1.shape} + {df2.shape} -> {merged_df.shape}\")\n",
    "\n",
    "            merged_dfs.append(merged_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Pair {i}: Error during merge - {e}\")\n",
    "            print(f\"Pair {i}: DF1 columns: {list(df1.columns) if 'df1' in locals() else 'N/A'}\")\n",
    "            print(f\"Pair {i}: DF2 columns: {list(df2.columns) if 'df2' in locals() else 'N/A'}\")\n",
    "            # Keep both original DataFrames if merge fails\n",
    "            merged_dfs.extend([df1, df2])\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\n=== MERGE SUMMARY ===\")\n",
    "    print(f\"Successfully processed: {len(merged_dfs)} DataFrames\")\n",
    "    print(f\"Total input pairs: {min(len(df_list1), len(df_list2))}\")\n",
    "\n",
    "    return merged_dfs"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "46b36be4b69b0272",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:48:00.624Z",
     "start_time": "2025-11-11T15:48:00.622227Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
